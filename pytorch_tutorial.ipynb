{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20837106",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912ff9b",
   "metadata": {},
   "source": [
    "Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e1447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb0d75",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d2c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(): initializes the random number generator (which needs a number to start with = seed value = 42 = joke)\n",
    "# Always initialize random seed to ensure reproducibility of results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Feature x is a vector with 100 points\n",
    "# rand(): creates an array of given shape and populates it with random samples from a uniform distribution over [0, 1)\n",
    "x = np.random.rand(100, 1)\n",
    "\n",
    "# Labels y from simple linear regression model (y = a + b*x + e, a = 1, b = 2, e = gaussian noise)\n",
    "# randn(): includes negative numbers\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d86667",
   "metadata": {},
   "source": [
    "Split into Train and Validation Set\n",
    "* Why is it necessary to generate indices and not shuffle feature x directly?x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d17bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices for feature x\n",
    "idx = np.arange(100)\n",
    "# Shuffle indices\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Use first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Use the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generate train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee862850",
   "metadata": {},
   "source": [
    "Implement Linear Regression\n",
    "* Epoch: is complete whenever every point has been already used for computing the loss.\n",
    "* Batch gradient descent: uses all points in the training set (N) to compute the loss (one epoch = one update)\n",
    "* Stochastic gradient descent: uses a single point at each time (one epoch = N updates)\n",
    "* Mini-batch gradient descent: uses anything else (n) in-between 1 and N (one epoch = N/n updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56fd7b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random a: [0.49671415] Random b: [-0.1382643]\n",
      "Updated a: [1.02354094] Updated b: [1.96896411]\n",
      "Sklearn a: [1.02354075] Sklearn b: [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(\"Random a:\", a, \"Random b:\", b)\n",
    "\n",
    "# Sets learning rate\n",
    "lr = 0.1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass: computes model's predicted output\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # Compute error\n",
    "    error = (y_train - yhat)\n",
    "    # Compute loss: regression problems often have mean squared error (MSE) as their loss\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Compute gradients: partial derivative for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Update parameters: use gradients and learning rate\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "    \n",
    "print(\"Updated a:\", a, \"Updated b:\", b)\n",
    "\n",
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(\"Sklearn a:\", linr.intercept_, \"Sklearn b:\", linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e57649",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314b727",
   "metadata": {},
   "source": [
    "Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f869f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e92d3",
   "metadata": {},
   "source": [
    "Trandormation from NumPy to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fa2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# cuda.is_available(): lets code fallback to CPU if no GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# from_numpy(): transforms NumPy arrays to PyTorch tensors\n",
    "# float(): casts tensors to a lower precision (32-bit float)\n",
    "# to(): sends tensors to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# .type(): tells where the tensor is\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99433618",
   "metadata": {},
   "source": [
    "Implement Linear Regression\n",
    "* Create Tensors used as trainable Parameters / Weights\n",
    "* Require the computation of its gradients to update their values \n",
    "* requires_grad = True: tells PyTorch we want it to compute gradients for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33048fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([-3.1125])\n",
      "tensor([-1.8156])\n",
      "tensor([-2.3184])\n",
      "tensor([-1.4064])\n",
      "tensor([-1.7219])\n",
      "tensor([-1.0982])\n",
      "tensor([-1.2737])\n",
      "tensor([-0.8659])\n",
      "tensor([-0.9372])\n",
      "tensor([-0.6906])\n",
      "tensor([-0.6845])\n",
      "tensor([-0.5583])\n",
      "tensor([-0.4948])\n",
      "tensor([-0.4582])\n",
      "tensor([-0.3526])\n",
      "tensor([-0.3824])\n",
      "tensor([-0.2459])\n",
      "tensor([-0.3248])\n",
      "tensor([-0.1660])\n",
      "tensor([-0.2810])\n",
      "tensor([-0.1063])\n",
      "tensor([-0.2475])\n",
      "tensor([-0.0616])\n",
      "tensor([-0.2218])\n",
      "tensor([-0.0283])\n",
      "tensor([-0.2019])\n",
      "tensor([-0.0036])\n",
      "tensor([-0.1864])\n",
      "tensor([0.0147])\n",
      "tensor([-0.1743])\n",
      "tensor([0.0283])\n",
      "tensor([-0.1646])\n",
      "tensor([0.0382])\n",
      "tensor([-0.1568])\n",
      "tensor([0.0453])\n",
      "tensor([-0.1505])\n",
      "tensor([0.0505])\n",
      "tensor([-0.1452])\n",
      "tensor([0.0541])\n",
      "tensor([-0.1408])\n",
      "tensor([0.0566])\n",
      "tensor([-0.1370])\n",
      "tensor([0.0582])\n",
      "tensor([-0.1337])\n",
      "tensor([0.0592])\n",
      "tensor([-0.1307])\n",
      "tensor([0.0597])\n",
      "tensor([-0.1280])\n",
      "tensor([0.0599])\n",
      "tensor([-0.1255])\n",
      "tensor([0.0598])\n",
      "tensor([-0.1232])\n",
      "tensor([0.0594])\n",
      "tensor([-0.1211])\n",
      "tensor([0.0590])\n",
      "tensor([-0.1190])\n",
      "tensor([0.0584])\n",
      "tensor([-0.1170])\n",
      "tensor([0.0578])\n",
      "tensor([-0.1151])\n",
      "tensor([0.0571])\n",
      "tensor([-0.1133])\n",
      "tensor([0.0564])\n",
      "tensor([-0.1115])\n",
      "tensor([0.0557])\n",
      "tensor([-0.1098])\n",
      "tensor([0.0549])\n",
      "tensor([-0.1081])\n",
      "tensor([0.0541])\n",
      "tensor([-0.1064])\n",
      "tensor([0.0534])\n",
      "tensor([-0.1048])\n",
      "tensor([0.0526])\n",
      "tensor([-0.1032])\n",
      "tensor([0.0518])\n",
      "tensor([-0.1016])\n",
      "tensor([0.0511])\n",
      "tensor([-0.1001])\n",
      "tensor([0.0503])\n",
      "tensor([-0.0985])\n",
      "tensor([0.0496])\n",
      "tensor([-0.0970])\n",
      "tensor([0.0488])\n",
      "tensor([-0.0956])\n",
      "tensor([0.0481])\n",
      "tensor([-0.0941])\n",
      "tensor([0.0474])\n",
      "tensor([-0.0927])\n",
      "tensor([0.0466])\n",
      "tensor([-0.0913])\n",
      "tensor([0.0459])\n",
      "tensor([-0.0899])\n",
      "tensor([0.0453])\n",
      "tensor([-0.0886])\n",
      "tensor([0.0446])\n",
      "tensor([-0.0872])\n",
      "tensor([0.0439])\n",
      "tensor([-0.0859])\n",
      "tensor([0.0432])\n",
      "tensor([-0.0846])\n",
      "tensor([0.0426])\n",
      "tensor([-0.0833])\n",
      "tensor([0.0419])\n",
      "tensor([-0.0821])\n",
      "tensor([0.0413])\n",
      "tensor([-0.0808])\n",
      "tensor([0.0407])\n",
      "tensor([-0.0796])\n",
      "tensor([0.0401])\n",
      "tensor([-0.0784])\n",
      "tensor([0.0395])\n",
      "tensor([-0.0772])\n",
      "tensor([0.0389])\n",
      "tensor([-0.0761])\n",
      "tensor([0.0383])\n",
      "tensor([-0.0749])\n",
      "tensor([0.0377])\n",
      "tensor([-0.0738])\n",
      "tensor([0.0371])\n",
      "tensor([-0.0727])\n",
      "tensor([0.0366])\n",
      "tensor([-0.0716])\n",
      "tensor([0.0360])\n",
      "tensor([-0.0705])\n",
      "tensor([0.0355])\n",
      "tensor([-0.0694])\n",
      "tensor([0.0349])\n",
      "tensor([-0.0684])\n",
      "tensor([0.0344])\n",
      "tensor([-0.0673])\n",
      "tensor([0.0339])\n",
      "tensor([-0.0663])\n",
      "tensor([0.0334])\n",
      "tensor([-0.0653])\n",
      "tensor([0.0329])\n",
      "tensor([-0.0643])\n",
      "tensor([0.0324])\n",
      "tensor([-0.0634])\n",
      "tensor([0.0319])\n",
      "tensor([-0.0624])\n",
      "tensor([0.0314])\n",
      "tensor([-0.0615])\n",
      "tensor([0.0309])\n",
      "tensor([-0.0605])\n",
      "tensor([0.0305])\n",
      "tensor([-0.0596])\n",
      "tensor([0.0300])\n",
      "tensor([-0.0587])\n",
      "tensor([0.0296])\n",
      "tensor([-0.0578])\n",
      "tensor([0.0291])\n",
      "tensor([-0.0570])\n",
      "tensor([0.0287])\n",
      "tensor([-0.0561])\n",
      "tensor([0.0282])\n",
      "tensor([-0.0552])\n",
      "tensor([0.0278])\n",
      "tensor([-0.0544])\n",
      "tensor([0.0274])\n",
      "tensor([-0.0536])\n",
      "tensor([0.0270])\n",
      "tensor([-0.0528])\n",
      "tensor([0.0266])\n",
      "tensor([-0.0520])\n",
      "tensor([0.0262])\n",
      "tensor([-0.0512])\n",
      "tensor([0.0258])\n",
      "tensor([-0.0504])\n",
      "tensor([0.0254])\n",
      "tensor([-0.0497])\n",
      "tensor([0.0250])\n",
      "tensor([-0.0489])\n",
      "tensor([0.0246])\n",
      "tensor([-0.0482])\n",
      "tensor([0.0242])\n",
      "tensor([-0.0474])\n",
      "tensor([0.0239])\n",
      "tensor([-0.0467])\n",
      "tensor([0.0235])\n",
      "tensor([-0.0460])\n",
      "tensor([0.0232])\n",
      "tensor([-0.0453])\n",
      "tensor([0.0228])\n",
      "tensor([-0.0446])\n",
      "tensor([0.0225])\n",
      "tensor([-0.0440])\n",
      "tensor([0.0221])\n",
      "tensor([-0.0433])\n",
      "tensor([0.0218])\n",
      "tensor([-0.0426])\n",
      "tensor([0.0215])\n",
      "tensor([-0.0420])\n",
      "tensor([0.0211])\n",
      "tensor([-0.0414])\n",
      "tensor([0.0208])\n",
      "tensor([-0.0407])\n",
      "tensor([0.0205])\n",
      "tensor([-0.0401])\n",
      "tensor([0.0202])\n",
      "tensor([-0.0395])\n",
      "tensor([0.0199])\n",
      "tensor([-0.0389])\n",
      "tensor([0.0196])\n",
      "tensor([-0.0383])\n",
      "tensor([0.0193])\n",
      "tensor([-0.0378])\n",
      "tensor([0.0190])\n",
      "tensor([-0.0372])\n",
      "tensor([0.0187])\n",
      "tensor([-0.0366])\n",
      "tensor([0.0184])\n",
      "tensor([-0.0361])\n",
      "tensor([0.0182])\n",
      "tensor([-0.0355])\n",
      "tensor([0.0179])\n",
      "tensor([-0.0350])\n",
      "tensor([0.0176])\n",
      "tensor([-0.0345])\n",
      "tensor([0.0173])\n",
      "tensor([-0.0339])\n",
      "tensor([0.0171])\n",
      "tensor([-0.0334])\n",
      "tensor([0.0168])\n",
      "tensor([-0.0329])\n",
      "tensor([0.0166])\n",
      "tensor([-0.0324])\n",
      "tensor([0.0163])\n",
      "tensor([-0.0319])\n",
      "tensor([0.0161])\n",
      "tensor([-0.0315])\n",
      "tensor([0.0158])\n",
      "tensor([-0.0310])\n",
      "tensor([0.0156])\n",
      "tensor([-0.0305])\n",
      "tensor([0.0154])\n",
      "tensor([-0.0301])\n",
      "tensor([0.0151])\n",
      "tensor([-0.0296])\n",
      "tensor([0.0149])\n",
      "tensor([-0.0291])\n",
      "tensor([0.0147])\n",
      "tensor([-0.0287])\n",
      "tensor([0.0145])\n",
      "tensor([-0.0283])\n",
      "tensor([0.0142])\n",
      "tensor([-0.0278])\n",
      "tensor([0.0140])\n",
      "tensor([-0.0274])\n",
      "tensor([0.0138])\n",
      "tensor([-0.0270])\n",
      "tensor([0.0136])\n",
      "tensor([-0.0266])\n",
      "tensor([0.0134])\n",
      "tensor([-0.0262])\n",
      "tensor([0.0132])\n",
      "tensor([-0.0258])\n",
      "tensor([0.0130])\n",
      "tensor([-0.0254])\n",
      "tensor([0.0128])\n",
      "tensor([-0.0250])\n",
      "tensor([0.0126])\n",
      "tensor([-0.0247])\n",
      "tensor([0.0124])\n",
      "tensor([-0.0243])\n",
      "tensor([0.0122])\n",
      "tensor([-0.0239])\n",
      "tensor([0.0120])\n",
      "tensor([-0.0236])\n",
      "tensor([0.0119])\n",
      "tensor([-0.0232])\n",
      "tensor([0.0117])\n",
      "tensor([-0.0228])\n",
      "tensor([0.0115])\n",
      "tensor([-0.0225])\n",
      "tensor([0.0113])\n",
      "tensor([-0.0222])\n",
      "tensor([0.0112])\n",
      "tensor([-0.0218])\n",
      "tensor([0.0110])\n",
      "tensor([-0.0215])\n",
      "tensor([0.0108])\n",
      "tensor([-0.0212])\n",
      "tensor([0.0107])\n",
      "tensor([-0.0209])\n",
      "tensor([0.0105])\n",
      "tensor([-0.0205])\n",
      "tensor([0.0103])\n",
      "tensor([-0.0202])\n",
      "tensor([0.0102])\n",
      "tensor([-0.0199])\n",
      "tensor([0.0100])\n",
      "tensor([-0.0196])\n",
      "tensor([0.0099])\n",
      "tensor([-0.0193])\n",
      "tensor([0.0097])\n",
      "tensor([-0.0190])\n",
      "tensor([0.0096])\n",
      "tensor([-0.0187])\n",
      "tensor([0.0094])\n",
      "tensor([-0.0185])\n",
      "tensor([0.0093])\n",
      "tensor([-0.0182])\n",
      "tensor([0.0092])\n",
      "tensor([-0.0179])\n",
      "tensor([0.0090])\n",
      "tensor([-0.0176])\n",
      "tensor([0.0089])\n",
      "tensor([-0.0174])\n",
      "tensor([0.0087])\n",
      "tensor([-0.0171])\n",
      "tensor([0.0086])\n",
      "tensor([-0.0169])\n",
      "tensor([0.0085])\n",
      "tensor([-0.0166])\n",
      "tensor([0.0084])\n",
      "tensor([-0.0163])\n",
      "tensor([0.0082])\n",
      "tensor([-0.0161])\n",
      "tensor([0.0081])\n",
      "tensor([-0.0159])\n",
      "tensor([0.0080])\n",
      "tensor([-0.0156])\n",
      "tensor([0.0079])\n",
      "tensor([-0.0154])\n",
      "tensor([0.0077])\n",
      "tensor([-0.0151])\n",
      "tensor([0.0076])\n",
      "tensor([-0.0149])\n",
      "tensor([0.0075])\n",
      "tensor([-0.0147])\n",
      "tensor([0.0074])\n",
      "tensor([-0.0145])\n",
      "tensor([0.0073])\n",
      "tensor([-0.0143])\n",
      "tensor([0.0072])\n",
      "tensor([-0.0140])\n",
      "tensor([0.0071])\n",
      "tensor([-0.0138])\n",
      "tensor([0.0070])\n",
      "tensor([-0.0136])\n",
      "tensor([0.0069])\n",
      "tensor([-0.0134])\n",
      "tensor([0.0068])\n",
      "tensor([-0.0132])\n",
      "tensor([0.0066])\n",
      "tensor([-0.0130])\n",
      "tensor([0.0065])\n",
      "tensor([-0.0128])\n",
      "tensor([0.0064])\n",
      "tensor([-0.0126])\n",
      "tensor([0.0064])\n",
      "tensor([-0.0124])\n",
      "tensor([0.0063])\n",
      "tensor([-0.0122])\n",
      "tensor([0.0062])\n",
      "tensor([-0.0121])\n",
      "tensor([0.0061])\n",
      "tensor([-0.0119])\n",
      "tensor([0.0060])\n",
      "tensor([-0.0117])\n",
      "tensor([0.0059])\n",
      "tensor([-0.0115])\n",
      "tensor([0.0058])\n",
      "tensor([-0.0113])\n",
      "tensor([0.0057])\n",
      "tensor([-0.0112])\n",
      "tensor([0.0056])\n",
      "tensor([-0.0110])\n",
      "tensor([0.0055])\n",
      "tensor([-0.0108])\n",
      "tensor([0.0055])\n",
      "tensor([-0.0107])\n",
      "tensor([0.0054])\n",
      "tensor([-0.0105])\n",
      "tensor([0.0053])\n",
      "tensor([-0.0104])\n",
      "tensor([0.0052])\n",
      "tensor([-0.0102])\n",
      "tensor([0.0051])\n",
      "tensor([-0.0100])\n",
      "tensor([0.0051])\n",
      "tensor([-0.0099])\n",
      "tensor([0.0050])\n",
      "tensor([-0.0097])\n",
      "tensor([0.0049])\n",
      "tensor([-0.0096])\n",
      "tensor([0.0048])\n",
      "tensor([-0.0094])\n",
      "tensor([0.0048])\n",
      "tensor([-0.0093])\n",
      "tensor([0.0047])\n",
      "tensor([-0.0092])\n",
      "tensor([0.0046])\n",
      "tensor([-0.0090])\n",
      "tensor([0.0045])\n",
      "tensor([-0.0089])\n",
      "tensor([0.0045])\n",
      "tensor([-0.0088])\n",
      "tensor([0.0044])\n",
      "tensor([-0.0086])\n",
      "tensor([0.0043])\n",
      "tensor([-0.0085])\n",
      "tensor([0.0043])\n",
      "tensor([-0.0084])\n",
      "tensor([0.0042])\n",
      "tensor([-0.0082])\n",
      "tensor([0.0041])\n",
      "tensor([-0.0081])\n",
      "tensor([0.0041])\n",
      "tensor([-0.0080])\n",
      "tensor([0.0040])\n",
      "tensor([-0.0079])\n",
      "tensor([0.0040])\n",
      "tensor([-0.0078])\n",
      "tensor([0.0039])\n",
      "tensor([-0.0076])\n",
      "tensor([0.0038])\n",
      "tensor([-0.0075])\n",
      "tensor([0.0038])\n",
      "tensor([-0.0074])\n",
      "tensor([0.0037])\n",
      "tensor([-0.0073])\n",
      "tensor([0.0037])\n",
      "tensor([-0.0072])\n",
      "tensor([0.0036])\n",
      "tensor([-0.0071])\n",
      "tensor([0.0036])\n",
      "tensor([-0.0070])\n",
      "tensor([0.0035])\n",
      "tensor([-0.0069])\n",
      "tensor([0.0035])\n",
      "tensor([-0.0068])\n",
      "tensor([0.0034])\n",
      "tensor([-0.0067])\n",
      "tensor([0.0034])\n",
      "tensor([-0.0066])\n",
      "tensor([0.0033])\n",
      "tensor([-0.0065])\n",
      "tensor([0.0033])\n",
      "tensor([-0.0064])\n",
      "tensor([0.0032])\n",
      "tensor([-0.0063])\n",
      "tensor([0.0032])\n",
      "tensor([-0.0062])\n",
      "tensor([0.0031])\n",
      "tensor([-0.0061])\n",
      "tensor([0.0031])\n",
      "tensor([-0.0060])\n",
      "tensor([0.0030])\n",
      "tensor([-0.0059])\n",
      "tensor([0.0030])\n",
      "tensor([-0.0058])\n",
      "tensor([0.0029])\n",
      "tensor([-0.0057])\n",
      "tensor([0.0029])\n",
      "tensor([-0.0056])\n",
      "tensor([0.0028])\n",
      "tensor([-0.0055])\n",
      "tensor([0.0028])\n",
      "tensor([-0.0055])\n",
      "tensor([0.0027])\n",
      "tensor([-0.0054])\n",
      "tensor([0.0027])\n",
      "tensor([-0.0053])\n",
      "tensor([0.0027])\n",
      "tensor([-0.0052])\n",
      "tensor([0.0026])\n",
      "tensor([-0.0051])\n",
      "tensor([0.0026])\n",
      "tensor([-0.0051])\n",
      "tensor([0.0025])\n",
      "tensor([-0.0050])\n",
      "tensor([0.0025])\n",
      "tensor([-0.0049])\n",
      "tensor([0.0025])\n",
      "tensor([-0.0048])\n",
      "tensor([0.0024])\n",
      "tensor([-0.0048])\n",
      "tensor([0.0024])\n",
      "tensor([-0.0047])\n",
      "tensor([0.0024])\n",
      "tensor([-0.0046])\n",
      "tensor([0.0023])\n",
      "tensor([-0.0046])\n",
      "tensor([0.0023])\n",
      "tensor([-0.0045])\n",
      "tensor([0.0023])\n",
      "tensor([-0.0044])\n",
      "tensor([0.0022])\n",
      "tensor([-0.0043])\n",
      "tensor([0.0022])\n",
      "tensor([-0.0043])\n",
      "tensor([0.0022])\n",
      "tensor([-0.0042])\n",
      "tensor([0.0021])\n",
      "tensor([-0.0042])\n",
      "tensor([0.0021])\n",
      "tensor([-0.0041])\n",
      "tensor([0.0021])\n",
      "tensor([-0.0040])\n",
      "tensor([0.0020])\n",
      "tensor([-0.0040])\n",
      "tensor([0.0020])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0039])\n",
      "tensor([0.0020])\n",
      "tensor([-0.0038])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0038])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0037])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0037])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0036])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0036])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0035])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0035])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0034])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0034])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0033])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0033])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0032])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0032])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0031])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0031])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0030])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0030])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0029])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0029])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0028])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0028])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0028])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0027])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0027])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0026])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0026])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0026])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0025])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0025])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0024])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0024])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0024])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0023])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0023])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0023])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0022])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0022])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0022])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0021])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0021])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0021])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0020])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0020])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0020])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0018])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0018])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0018])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([9.9593e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.7900e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.6528e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.5082e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.3635e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.2131e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.0877e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.9555e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.8250e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.6921e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.5517e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.4098e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.2713e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.1572e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.0483e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.9146e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.8050e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.6955e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.5683e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.4612e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.3319e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.2197e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.1033e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.9905e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.8819e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.7824e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.6845e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.5868e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.4931e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.4093e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.3057e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.2128e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.1230e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.0353e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.9323e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.8341e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.7453e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.6731e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.5748e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.4914e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.4108e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.3403e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.2583e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.1776e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.1086e-05])\n",
      "tensor([-9.9640e-05])\n",
      "tensor([5.0306e-05])\n",
      "tensor([-9.8126e-05])\n",
      "tensor([4.9496e-05])\n",
      "tensor([-9.6684e-05])\n",
      "tensor([4.8649e-05])\n",
      "tensor([-9.5258e-05])\n",
      "tensor([4.7850e-05])\n",
      "tensor([-9.3862e-05])\n",
      "tensor([4.7218e-05])\n",
      "tensor([-9.2392e-05])\n",
      "tensor([4.6494e-05])\n",
      "tensor([-9.0986e-05])\n",
      "tensor([4.5732e-05])\n",
      "tensor([-8.9653e-05])\n",
      "tensor([4.5110e-05])\n",
      "tensor([-8.8274e-05])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.4386e-05])\n",
      "tensor([-8.6958e-05])\n",
      "tensor([4.3813e-05])\n",
      "tensor([-8.5593e-05])\n",
      "tensor([4.3080e-05])\n",
      "tensor([-8.4333e-05])\n",
      "tensor([4.2503e-05])\n",
      "tensor([-8.3017e-05])\n",
      "tensor([4.1799e-05])\n",
      "tensor([-8.1784e-05])\n",
      "tensor([4.1258e-05])\n",
      "tensor([-8.0489e-05])\n",
      "tensor([4.0557e-05])\n",
      "tensor([-7.9301e-05])\n",
      "tensor([3.9983e-05])\n",
      "tensor([-7.8079e-05])\n",
      "tensor([3.9206e-05])\n",
      "tensor([-7.6996e-05])\n",
      "tensor([3.8651e-05])\n",
      "tensor([-7.5801e-05])\n",
      "tensor([3.8225e-05])\n",
      "tensor([-7.4577e-05])\n",
      "tensor([3.7706e-05])\n",
      "tensor([-7.3411e-05])\n",
      "tensor([3.7102e-05])\n",
      "tensor([-7.2300e-05])\n",
      "tensor([3.6565e-05])\n",
      "tensor([-7.1189e-05])\n",
      "tensor([3.5922e-05])\n",
      "tensor([-7.0154e-05])\n",
      "tensor([3.5395e-05])\n",
      "tensor([-6.9097e-05])\n",
      "tensor([3.4794e-05])\n",
      "tensor([-6.8070e-05])\n",
      "tensor([3.4280e-05])\n",
      "tensor([-6.7045e-05])\n",
      "tensor([3.3686e-05])\n",
      "tensor([-6.6071e-05])\n",
      "tensor([3.3202e-05])\n",
      "tensor([-6.5076e-05])\n",
      "tensor([3.2708e-05])\n",
      "tensor([-6.4071e-05])\n",
      "tensor([3.2351e-05])\n",
      "tensor([-6.3037e-05])\n",
      "tensor([3.1894e-05])\n",
      "tensor([-6.2072e-05])\n",
      "tensor([3.1340e-05])\n",
      "tensor([-6.1164e-05])\n",
      "tensor([3.0885e-05])\n",
      "tensor([-6.0245e-05])\n",
      "tensor([3.0420e-05])\n",
      "tensor([-5.9325e-05])\n",
      "tensor([2.9830e-05])\n",
      "tensor([-5.8491e-05])\n",
      "tensor([2.9391e-05])\n",
      "tensor([-5.7597e-05])\n",
      "tensor([2.8838e-05])\n",
      "tensor([-5.6787e-05])\n",
      "tensor([2.8547e-05])\n",
      "tensor([-5.5845e-05])\n",
      "tensor([2.8067e-05])\n",
      "tensor([-5.5037e-05])\n",
      "tensor([2.7576e-05])\n",
      "tensor([-5.4227e-05])\n",
      "tensor([2.7156e-05])\n",
      "tensor([-5.3419e-05])\n",
      "tensor([2.6734e-05])\n",
      "tensor([-5.2610e-05])\n",
      "tensor([2.6423e-05])\n",
      "tensor([-5.1769e-05])\n",
      "tensor([2.6060e-05])\n",
      "tensor([-5.0979e-05])\n",
      "tensor([2.5657e-05])\n",
      "tensor([-5.0212e-05])\n",
      "tensor([2.5140e-05])\n",
      "tensor([-4.9514e-05])\n",
      "tensor([2.4864e-05])\n",
      "tensor([-4.8710e-05])\n",
      "tensor([2.4471e-05])\n",
      "tensor([-4.7985e-05])\n",
      "tensor([2.3978e-05])\n",
      "tensor([-4.7320e-05])\n",
      "tensor([2.3684e-05])\n",
      "tensor([-4.6571e-05])\n",
      "tensor([2.3326e-05])\n",
      "tensor([-4.5856e-05])\n",
      "tensor([2.2852e-05])\n",
      "tensor([-4.5239e-05])\n",
      "tensor([2.2600e-05])\n",
      "tensor([-4.4504e-05])\n",
      "tensor([2.2241e-05])\n",
      "tensor([-4.3848e-05])\n",
      "tensor([2.1889e-05])\n",
      "tensor([-4.3181e-05])\n",
      "tensor([2.1611e-05])\n",
      "tensor([-4.2512e-05])\n",
      "tensor([2.1398e-05])\n",
      "tensor([-4.1804e-05])\n",
      "tensor([2.1037e-05])\n",
      "tensor([-4.1199e-05])\n",
      "tensor([2.0692e-05])\n",
      "tensor([-4.0582e-05])\n",
      "tensor([2.0462e-05])\n",
      "tensor([-3.9930e-05])\n",
      "tensor([2.0137e-05])\n",
      "tensor([-3.9340e-05])\n",
      "tensor([1.9828e-05])\n",
      "tensor([-3.8749e-05])\n",
      "tensor([1.9468e-05])\n",
      "tensor([-3.8180e-05])\n",
      "tensor([1.9222e-05])\n",
      "tensor([-3.7587e-05])\n",
      "tensor([1.9011e-05])\n",
      "tensor([-3.6970e-05])\n",
      "tensor([1.8727e-05])\n",
      "tensor([-3.6407e-05])\n",
      "tensor([1.8391e-05])\n",
      "tensor([-3.5875e-05])\n",
      "tensor([1.8169e-05])\n",
      "tensor([-3.5314e-05])\n",
      "tensor([1.7992e-05])\n",
      "tensor([-3.4718e-05])\n",
      "tensor([1.7693e-05])\n",
      "tensor([-3.4219e-05])\n",
      "tensor([1.7415e-05])\n",
      "tensor([-3.3694e-05])\n",
      "tensor([1.6961e-05])\n",
      "tensor([-3.3285e-05])\n",
      "tensor([1.6763e-05])\n",
      "tensor([-3.2756e-05])\n",
      "tensor([1.6480e-05])\n",
      "tensor([-3.2284e-05])\n",
      "tensor([1.6171e-05])\n",
      "tensor([-3.1832e-05])\n",
      "tensor([1.5910e-05])\n",
      "tensor([-3.1346e-05])\n",
      "tensor([1.5727e-05])\n",
      "tensor([-3.0851e-05])\n",
      "tensor([1.5564e-05])\n",
      "tensor([-3.0346e-05])\n",
      "tensor([1.5290e-05])\n",
      "tensor([-2.9914e-05])\n",
      "tensor([1.4984e-05])\n",
      "tensor([-2.9495e-05])\n",
      "tensor([1.4712e-05])\n",
      "tensor([-2.9066e-05])\n",
      "tensor([1.4538e-05])\n",
      "tensor([-2.8613e-05])\n",
      "tensor([1.4408e-05])\n",
      "tensor([-2.8139e-05])\n",
      "tensor([1.4248e-05])\n",
      "tensor([-2.7681e-05])\n",
      "tensor([1.3953e-05])\n",
      "tensor([-2.7312e-05])\n",
      "tensor([1.3675e-05])\n",
      "tensor([-2.6926e-05])\n",
      "tensor([1.3649e-05])\n",
      "tensor([-2.6420e-05])\n",
      "tensor([1.3521e-05])\n",
      "tensor([-2.5995e-05])\n",
      "tensor([1.3351e-05])\n",
      "tensor([-2.5576e-05])\n",
      "tensor([1.3142e-05])\n",
      "tensor([-2.5200e-05])\n",
      "tensor([1.2852e-05])\n",
      "tensor([-2.4874e-05])\n",
      "tensor([1.2607e-05])\n",
      "tensor([-2.4519e-05])\n",
      "tensor([1.2361e-05])\n",
      "tensor([-2.4165e-05])\n",
      "tensor([1.2212e-05])\n",
      "tensor([-2.3796e-05])\n",
      "tensor([1.2074e-05])\n",
      "tensor([-2.3412e-05])\n",
      "tensor([1.1940e-05])\n",
      "tensor([-2.3022e-05])\n",
      "tensor([1.1697e-05])\n",
      "tensor([-2.2717e-05])\n",
      "tensor([1.1452e-05])\n",
      "tensor([-2.2410e-05])\n",
      "tensor([1.1185e-05])\n",
      "tensor([-2.2110e-05])\n",
      "tensor([1.1190e-05])\n",
      "tensor([-2.1685e-05])\n",
      "tensor([1.1090e-05])\n",
      "tensor([-2.1328e-05])\n",
      "tensor([1.0995e-05])\n",
      "tensor([-2.0965e-05])\n",
      "tensor([1.0843e-05])\n",
      "tensor([-2.0638e-05])\n",
      "tensor([1.0596e-05])\n",
      "tensor([-2.0378e-05])\n",
      "tensor([1.0381e-05])\n",
      "tensor([-2.0099e-05])\n",
      "tensor([1.0169e-05])\n",
      "tensor([-1.9810e-05])\n",
      "tensor([9.9139e-06])\n",
      "tensor([-1.9562e-05])\n",
      "tensor([9.8259e-06])\n",
      "tensor([-1.9239e-05])\n",
      "tensor([9.7388e-06])\n",
      "tensor([-1.8920e-05])\n",
      "tensor([9.5954e-06])\n",
      "tensor([-1.8628e-05])\n",
      "tensor([9.5186e-06])\n",
      "tensor([-1.8313e-05])\n",
      "tensor([9.2844e-06])\n",
      "tensor([-1.8095e-05])\n",
      "tensor([9.0720e-06])\n",
      "tensor([-1.7857e-05])\n",
      "tensor([8.8690e-06])\n",
      "tensor([-1.7605e-05])\n",
      "tensor([8.8969e-06])\n",
      "tensor([-1.7261e-05])\n",
      "tensor([8.7656e-06])\n",
      "tensor([-1.7018e-05])\n",
      "tensor([8.7041e-06])\n",
      "tensor([-1.6719e-05])\n",
      "tensor([8.5849e-06])\n",
      "tensor([-1.6477e-05])\n",
      "tensor([8.4946e-06])\n",
      "tensor([-1.6209e-05])\n",
      "tensor([8.4154e-06])\n",
      "tensor([-1.5922e-05])\n",
      "tensor([8.2180e-06])\n",
      "tensor([-1.5732e-05])\n",
      "tensor([8.0187e-06])\n",
      "tensor([-1.5523e-05])\n",
      "tensor([7.8045e-06])\n",
      "tensor([-1.5342e-05])\n",
      "tensor([7.5679e-06])\n",
      "tensor([-1.5164e-05])\n",
      "tensor([7.6322e-06])\n",
      "tensor([-1.4845e-05])\n",
      "tensor([7.5605e-06])\n",
      "tensor([-1.4611e-05])\n",
      "tensor([7.4748e-06])\n",
      "tensor([-1.4379e-05])\n",
      "tensor([7.3779e-06])\n",
      "tensor([-1.4175e-05])\n",
      "tensor([7.3090e-06])\n",
      "tensor([-1.3923e-05])\n",
      "tensor([7.2317e-06])\n",
      "tensor([-1.3692e-05])\n",
      "tensor([7.0189e-06])\n",
      "tensor([-1.3553e-05])\n",
      "tensor([6.8340e-06])\n",
      "tensor([-1.3394e-05])\n",
      "tensor([6.6645e-06])\n",
      "tensor([-1.3220e-05])\n",
      "tensor([6.4904e-06])\n",
      "tensor([-1.3053e-05])\n",
      "tensor([6.4960e-06])\n",
      "tensor([-1.2812e-05])\n",
      "tensor([6.5668e-06])\n",
      "tensor([-1.2529e-05])\n",
      "tensor([6.3819e-06])\n",
      "tensor([-1.2364e-05])\n",
      "tensor([6.2883e-06])\n",
      "tensor([-1.2195e-05])\n",
      "tensor([6.2520e-06])\n",
      "tensor([-1.1981e-05])\n",
      "tensor([6.1919e-06])\n",
      "tensor([-1.1794e-05])\n",
      "tensor([6.1188e-06])\n",
      "tensor([-1.1604e-05])\n",
      "tensor([6.0387e-06])\n",
      "tensor([-1.1412e-05])\n",
      "tensor([5.9716e-06])\n",
      "tensor([-1.1222e-05])\n",
      "tensor([5.8031e-06])\n",
      "tensor([-1.1110e-05])\n",
      "tensor([5.6336e-06])\n",
      "tensor([-1.0979e-05])\n",
      "tensor([5.4603e-06])\n",
      "tensor([-1.0857e-05])\n",
      "tensor([5.2704e-06])\n",
      "tensor([-1.0748e-05])\n",
      "tensor([5.3230e-06])\n",
      "tensor([-1.0535e-05])\n",
      "tensor([5.3840e-06])\n",
      "tensor([-1.0292e-05])\n",
      "tensor([5.1949e-06])\n",
      "tensor([-1.0185e-05])\n",
      "tensor([5.2750e-06])\n",
      "tensor([-9.9503e-06])\n",
      "tensor([5.2582e-06])\n",
      "tensor([-9.7628e-06])\n",
      "tensor([5.1539e-06])\n",
      "tensor([-9.6364e-06])\n",
      "tensor([5.1362e-06])\n",
      "tensor([-9.4608e-06])\n",
      "tensor([5.0925e-06])\n",
      "tensor([-9.3081e-06])\n",
      "tensor([4.9807e-06])\n",
      "tensor([-9.1949e-06])\n",
      "tensor([4.9304e-06])\n",
      "tensor([-9.0390e-06])\n",
      "tensor([4.8950e-06])\n",
      "tensor([-8.8778e-06])\n",
      "tensor([4.7348e-06])\n",
      "tensor([-8.7903e-06])\n",
      "tensor([4.5747e-06])\n",
      "tensor([-8.7037e-06])\n",
      "tensor([4.4145e-06])\n",
      "tensor([-8.6257e-06])\n",
      "tensor([4.2357e-06])\n",
      "tensor([-8.5547e-06])\n",
      "tensor([4.0680e-06])\n",
      "tensor([-8.4816e-06])\n",
      "tensor([4.1481e-06])\n",
      "tensor([-8.2841e-06])\n",
      "tensor([4.2226e-06])\n",
      "tensor([-8.0937e-06])\n",
      "tensor([4.0773e-06])\n",
      "tensor([-8.0033e-06])\n",
      "tensor([4.1304e-06])\n",
      "tensor([-7.8185e-06])\n",
      "tensor([4.2040e-06])\n",
      "tensor([-7.6303e-06])\n",
      "tensor([3.9209e-06])\n",
      "tensor([-7.6341e-06])\n",
      "tensor([3.8939e-06])\n",
      "tensor([-7.5102e-06])\n",
      "tensor([3.8566e-06])\n",
      "tensor([-7.3886e-06])\n",
      "tensor([3.7849e-06])\n",
      "tensor([-7.3011e-06])\n",
      "tensor([3.7821e-06])\n",
      "tensor([-7.1619e-06])\n",
      "tensor([3.7216e-06])\n",
      "tensor([-7.0529e-06])\n",
      "tensor([3.6652e-06])\n",
      "tensor([-6.9528e-06])\n",
      "tensor([3.6694e-06])\n",
      "tensor([-6.8131e-06])\n",
      "tensor([3.6312e-06])\n",
      "tensor([-6.6911e-06])\n",
      "tensor([3.5805e-06])\n",
      "tensor([-6.5877e-06])\n",
      "tensor([3.5302e-06])\n",
      "tensor([-6.4708e-06])\n",
      "tensor([3.3751e-06])\n",
      "tensor([-6.4364e-06])\n",
      "tensor([3.2438e-06])\n",
      "tensor([-6.3798e-06])\n",
      "tensor([3.0673e-06])\n",
      "tensor([-6.3649e-06])\n",
      "tensor([2.9169e-06])\n",
      "tensor([-6.3283e-06])\n",
      "tensor([3.0063e-06])\n",
      "tensor([-6.1644e-06])\n",
      "tensor([2.8526e-06])\n",
      "tensor([-6.1288e-06])\n",
      "tensor([2.9271e-06])\n",
      "tensor([-5.9761e-06])\n",
      "tensor([3.0342e-06])\n",
      "tensor([-5.8163e-06])\n",
      "tensor([2.9062e-06])\n",
      "tensor([-5.7574e-06])\n",
      "tensor([2.9374e-06])\n",
      "tensor([-5.6359e-06])\n",
      "tensor([3.0212e-06])\n",
      "tensor([-5.4925e-06])\n",
      "tensor([2.9062e-06])\n",
      "tensor([-5.4231e-06])\n",
      "tensor([2.9700e-06])\n",
      "tensor([-5.2867e-06])\n",
      "tensor([2.9458e-06])\n",
      "tensor([-5.2082e-06])\n",
      "tensor([2.9090e-06])\n",
      "tensor([-5.1397e-06])\n",
      "tensor([2.9271e-06])\n",
      "tensor([-5.0352e-06])\n",
      "tensor([2.8824e-06])\n",
      "tensor([-4.9761e-06])\n",
      "tensor([2.8471e-06])\n",
      "tensor([-4.8955e-06])\n",
      "tensor([2.8368e-06])\n",
      "tensor([-4.8168e-06])\n",
      "tensor([2.7912e-06])\n",
      "tensor([-4.7437e-06])\n",
      "tensor([2.7595e-06])\n",
      "tensor([-4.6727e-06])\n",
      "tensor([2.7707e-06])\n",
      "tensor([-4.5723e-06])\n",
      "tensor([2.7129e-06])\n",
      "tensor([-4.5155e-06])\n",
      "tensor([2.6934e-06])\n",
      "tensor([-4.4457e-06])\n",
      "tensor([2.6664e-06])\n",
      "tensor([-4.3549e-06])\n",
      "tensor([2.6403e-06])\n",
      "tensor([-4.2808e-06])\n",
      "tensor([2.6189e-06])\n",
      "tensor([-4.1961e-06])\n",
      "tensor([2.5863e-06])\n",
      "tensor([-4.1244e-06])\n",
      "tensor([2.4531e-06])\n",
      "tensor([-4.1206e-06])\n",
      "tensor([2.3218e-06])\n",
      "tensor([-4.1244e-06])\n",
      "tensor([2.1812e-06])\n",
      "tensor([-4.1162e-06])\n",
      "tensor([2.0419e-06])\n",
      "tensor([-4.1134e-06])\n",
      "tensor([1.9101e-06])\n",
      "tensor([-4.1104e-06])\n",
      "tensor([1.7472e-06])\n",
      "tensor([-4.1234e-06])\n",
      "tensor([1.8640e-06])\n",
      "tensor([-3.9954e-06])\n",
      "tensor([1.7462e-06])\n",
      "tensor([-3.9786e-06])\n",
      "tensor([1.7886e-06])\n",
      "tensor([-3.9116e-06])\n",
      "tensor([1.6550e-06])\n",
      "tensor([-3.9032e-06])\n",
      "tensor([1.7961e-06])\n",
      "tensor([-3.7518e-06])\n",
      "tensor([1.6307e-06])\n",
      "tensor([-3.7816e-06])\n",
      "tensor([1.7220e-06])\n",
      "tensor([-3.6587e-06])\n",
      "tensor([1.8515e-06])\n",
      "tensor([-3.5283e-06])\n",
      "tensor([1.7015e-06])\n",
      "tensor([-3.5372e-06])\n",
      "tensor([1.8058e-06])\n",
      "tensor([-3.4194e-06])\n",
      "tensor([1.6391e-06])\n",
      "tensor([-3.4361e-06])\n",
      "tensor([1.7486e-06])\n",
      "tensor([-3.3178e-06])\n",
      "tensor([1.8612e-06])\n",
      "tensor([-3.1949e-06])\n",
      "tensor([1.7034e-06])\n",
      "tensor([-3.2072e-06])\n",
      "tensor([1.8179e-06])\n",
      "tensor([-3.0810e-06])\n",
      "tensor([1.6848e-06])\n",
      "tensor([-3.0710e-06])\n",
      "tensor([1.7881e-06])\n",
      "tensor([-2.9621e-06])\n",
      "tensor([1.5330e-06])\n",
      "tensor([-3.0368e-06])\n",
      "tensor([1.6419e-06])\n",
      "tensor([-2.9120e-06])\n",
      "tensor([1.6047e-06])\n",
      "tensor([-2.8966e-06])\n",
      "tensor([1.5721e-06])\n",
      "tensor([-2.8734e-06])\n",
      "tensor([1.6280e-06])\n",
      "tensor([-2.7746e-06])\n",
      "tensor([1.5995e-06])\n",
      "tensor([-2.7441e-06])\n",
      "tensor([1.5688e-06])\n",
      "tensor([-2.7178e-06])\n",
      "tensor([1.5292e-06])\n",
      "tensor([-2.7108e-06])\n",
      "tensor([1.5148e-06])\n",
      "tensor([-2.6741e-06])\n",
      "tensor([1.5334e-06])\n",
      "tensor([-2.6140e-06])\n",
      "tensor([1.5250e-06])\n",
      "tensor([-2.5670e-06])\n",
      "tensor([1.5134e-06])\n",
      "tensor([-2.5269e-06])\n",
      "tensor([1.4771e-06])\n",
      "tensor([-2.5083e-06])\n",
      "tensor([1.5050e-06])\n",
      "tensor([-2.4401e-06])\n",
      "tensor([1.4761e-06])\n",
      "tensor([-2.4112e-06])\n",
      "tensor([1.4724e-06])\n",
      "tensor([-2.3642e-06])\n",
      "tensor([1.4352e-06])\n",
      "tensor([-2.3390e-06])\n",
      "tensor([1.4161e-06])\n",
      "tensor([-2.3087e-06])\n",
      "tensor([1.4072e-06])\n",
      "tensor([-2.2666e-06])\n",
      "tensor([1.3839e-06])\n",
      "tensor([-2.2347e-06])\n",
      "tensor([1.3709e-06])\n",
      "tensor([-2.1968e-06])\n",
      "tensor([1.3579e-06])\n",
      "tensor([-2.1590e-06])\n",
      "tensor([1.3672e-06])\n",
      "tensor([-2.1036e-06])\n",
      "tensor([1.3448e-06])\n",
      "tensor([-2.0689e-06])\n",
      "tensor([1.3271e-06])\n",
      "tensor([-2.0352e-06])\n",
      "tensor([1.2871e-06])\n",
      "tensor([-2.0219e-06])\n",
      "tensor([1.3132e-06])\n",
      "tensor([-1.9546e-06])\n",
      "tensor([1.2787e-06])\n",
      "tensor([-1.9302e-06])\n",
      "tensor([1.2573e-06])\n",
      "tensor([-1.8957e-06])\n",
      "tensor([1.2433e-06])\n",
      "tensor([-1.8571e-06])\n",
      "tensor([1.2196e-06])\n",
      "tensor([-1.8277e-06])\n",
      "tensor([1.2149e-06])\n",
      "tensor([-1.7839e-06])\n",
      "tensor([1.1073e-06])\n",
      "tensor([-1.8110e-06])\n",
      "tensor([1.0990e-06])\n",
      "tensor([-1.7639e-06])\n",
      "tensor([9.7509e-07])\n",
      "tensor([-1.7984e-06])\n",
      "tensor([9.7742e-07])\n",
      "tensor([-1.7588e-06])\n",
      "tensor([8.0653e-07])\n",
      "tensor([-1.8226e-06])\n",
      "tensor([8.1677e-07])\n",
      "tensor([-1.7639e-06])\n",
      "tensor([6.8732e-07])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8040e-06])\n",
      "tensor([6.6869e-07])\n",
      "tensor([-1.7700e-06])\n",
      "tensor([5.7090e-07])\n",
      "tensor([-1.7958e-06])\n",
      "tensor([7.9349e-07])\n",
      "tensor([-1.6377e-06])\n",
      "tensor([6.8545e-07])\n",
      "tensor([-1.6615e-06])\n",
      "tensor([5.5367e-07])\n",
      "tensor([-1.7004e-06])\n",
      "tensor([6.2864e-07])\n",
      "tensor([-1.6503e-06])\n",
      "tensor([5.2480e-07])\n",
      "tensor([-1.6752e-06])\n",
      "tensor([6.0443e-07])\n",
      "tensor([-1.6293e-06])\n",
      "tensor([5.0245e-07])\n",
      "tensor([-1.6543e-06])\n",
      "tensor([5.9791e-07])\n",
      "tensor([-1.5893e-06])\n",
      "tensor([4.6007e-07])\n",
      "tensor([-1.6298e-06])\n",
      "tensor([6.1467e-07])\n",
      "tensor([-1.5278e-06])\n",
      "tensor([4.5588e-07])\n",
      "tensor([-1.5809e-06])\n",
      "tensor([5.9651e-07])\n",
      "tensor([-1.4883e-06])\n",
      "tensor([4.4564e-07])\n",
      "tensor([-1.5348e-06])\n",
      "tensor([5.7835e-07])\n",
      "tensor([-1.4510e-06])\n",
      "tensor([7.1572e-07])\n",
      "tensor([-1.3693e-06])\n",
      "tensor([5.7556e-07])\n",
      "tensor([-1.4133e-06])\n",
      "tensor([6.8732e-07])\n",
      "tensor([-1.3316e-06])\n",
      "tensor([5.7463e-07])\n",
      "tensor([-1.3630e-06])\n",
      "tensor([6.6590e-07])\n",
      "tensor([-1.3034e-06])\n",
      "tensor([5.4063e-07])\n",
      "tensor([-1.3341e-06])\n",
      "tensor([6.3702e-07])\n",
      "tensor([-1.2727e-06])\n",
      "tensor([5.2061e-07])\n",
      "tensor([-1.2992e-06])\n",
      "tensor([6.1840e-07])\n",
      "tensor([-1.2354e-06])\n",
      "tensor([5.0012e-07])\n",
      "tensor([-1.2638e-06])\n",
      "tensor([6.1281e-07])\n",
      "tensor([-1.1921e-06])\n",
      "tensor([4.8708e-07])\n",
      "tensor([-1.2342e-06])\n",
      "tensor([6.0024e-07])\n",
      "tensor([-1.1525e-06])\n",
      "tensor([4.8056e-07])\n",
      "tensor([-1.1898e-06])\n",
      "tensor([5.9884e-07])\n",
      "tensor([-1.1041e-06])\n",
      "tensor([4.7404e-07])\n",
      "tensor([-1.1437e-06])\n",
      "tensor([5.8813e-07])\n",
      "tensor([-1.0631e-06])\n",
      "tensor([6.9570e-07])\n",
      "tensor([-1.0040e-06])\n",
      "tensor([5.5507e-07])\n",
      "tensor([-1.0435e-06])\n",
      "tensor([6.6217e-07])\n",
      "tensor([-9.6997e-07])\n",
      "tensor([5.3365e-07])\n",
      "tensor([-1.0058e-06])\n",
      "tensor([6.4354e-07])\n",
      "tensor([-9.3086e-07])\n",
      "tensor([5.2573e-07])\n",
      "tensor([-9.6788e-07])\n",
      "tensor([6.5658e-07])\n",
      "tensor([-8.7451e-07])\n",
      "tensor([5.4901e-07])\n",
      "tensor([-9.0571e-07])\n",
      "tensor([6.3889e-07])\n",
      "tensor([-8.4029e-07])\n",
      "tensor([5.0990e-07])\n",
      "tensor([-8.8336e-07])\n",
      "tensor([6.2678e-07])\n",
      "tensor([-8.0187e-07])\n",
      "tensor([5.2666e-07])\n",
      "tensor([-8.2375e-07])\n",
      "tensor([6.2026e-07])\n",
      "tensor([-7.6601e-07])\n",
      "tensor([4.8056e-07])\n",
      "tensor([-8.0629e-07])\n",
      "tensor([5.8440e-07])\n",
      "tensor([-7.2992e-07])\n",
      "tensor([7.1572e-07])\n",
      "tensor([-6.4168e-07])\n",
      "tensor([5.9046e-07])\n",
      "tensor([-6.8313e-07])\n",
      "tensor([6.7102e-07])\n",
      "tensor([-6.2911e-07])\n",
      "tensor([5.8115e-07])\n",
      "tensor([-6.5123e-07])\n",
      "tensor([6.8545e-07])\n",
      "tensor([-5.7556e-07])\n",
      "tensor([4.5449e-07])\n",
      "tensor([-6.8825e-07])\n",
      "tensor([5.7463e-07])\n",
      "tensor([-6.0257e-07])\n",
      "tensor([6.7707e-07])\n",
      "tensor([-5.3365e-07])\n",
      "tensor([4.3865e-07])\n",
      "tensor([-6.4494e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n",
      "tensor([5.4855e-07])\n",
      "tensor([-5.7323e-07])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Same as seed() for NumPy\n",
    "torch.manual_seed(42)\n",
    "# Specify device at the moment of creation\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "print(a, b)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # Tell PyTorch to work its way BACKWARDS from the specified loss\n",
    "    loss.backward()\n",
    "    # Check the computed gradients\n",
    "    print(a.grad)\n",
    "    print(b.grad)\n",
    "    \n",
    "    # UPDATING the parameters\n",
    "    \n",
    "    # FIRST ATTEMPT\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # a = a - lr * a.grad\n",
    "    # b = b - lr * b.grad\n",
    "    # print(a)\n",
    "\n",
    "    # SECOND ATTEMPT\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # a -= lr * a.grad\n",
    "    # b -= lr * b.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT\n",
    "    # We need to use no_grad() to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2002e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07209b4d",
   "metadata": {},
   "source": [
    "Dynamic Computation Graph\n",
    "* Blue boxes: correspond to the tensors used as parameters\n",
    "* Gray boxes: correspond to Python operations involving a gradient-computing tensor or its dependencies\n",
    "* Green boxes: same as the gray boxes, except it is the starting point for the computation of gradients (assuming backward() is called from the variable used to visualize the graph)  they are computed from the bottom-up in a graph.\n",
    "* Even though there are more tensors involved in the operations performed by the computation graph, it only shows gradient-computing tensors and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e376c892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 3.0.0 (20220226.1711)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"283pt\"\n",
       " viewBox=\"0.00 0.00 222.00 283.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-279 218,-279 218,4 -4,4\"/>\n",
       "<!-- 5017002976 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5017002976</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"139,-31 74,-31 74,0 139,0 139,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n",
       "</g>\n",
       "<!-- 5016838032 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5016838032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 5016838032&#45;&gt;5017002976 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5016838032&#45;&gt;5017002976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n",
       "</g>\n",
       "<!-- 5016845328 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5016845328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5016845328&#45;&gt;5016838032 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5016845328&#45;&gt;5016838032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n",
       "</g>\n",
       "<!-- 5017002656 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5017002656</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-208 23.5,-208 23.5,-177 77.5,-177 77.5,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 5017002656&#45;&gt;5016845328 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5017002656&#45;&gt;5016845328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.92C50.5,-169.22 50.5,-159.69 50.5,-151.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.25 50.5,-141.25 47,-151.25 54,-151.25\"/>\n",
       "</g>\n",
       "<!-- 5016836112 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5016836112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 5016836112&#45;&gt;5016838032 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5016836112&#45;&gt;5016838032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n",
       "</g>\n",
       "<!-- 5016845664 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5016845664</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-202 113,-202 113,-183 214,-183 214,-202\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-190\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5016845664&#45;&gt;5016836112 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5016845664&#45;&gt;5016836112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.79C163.5,-174.6 163.5,-162.06 163.5,-151.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.24 163.5,-141.24 160,-151.24 167,-151.24\"/>\n",
       "</g>\n",
       "<!-- 5017004816 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5017004816</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-275 136.5,-275 136.5,-244 190.5,-244 190.5,-275\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 5017004816&#45;&gt;5016845664 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5017004816&#45;&gt;5016845664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-243.75C163.5,-234.39 163.5,-222.19 163.5,-212.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-212.02 163.5,-202.02 160,-212.02 167,-212.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x12b06f160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Visualize a graph associated with yhat\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707b11b",
   "metadata": {},
   "source": [
    "Implement Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce1d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Define SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()    \n",
    "    \n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8761e6d",
   "metadata": {},
   "source": [
    "Implement Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a419f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines an MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cce13",
   "metadata": {},
   "source": [
    "Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daea0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, a model is represented by a regular Python class that inherits from the Module class\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    # Define the parts that make up the model  in our case, two parameters, a and b\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f4c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device (same device as data!)\n",
    "model = ManualLinearRegression().to(device)\n",
    "# We can also inspect its current parameters using its state_dict()\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Does NOT perform a training step\n",
    "    # Sets the model to training mode (relevant if distinct behaviors in training and evaluation phases appear)\n",
    "    model.train()\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00be68",
   "metadata": {},
   "source": [
    "### Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c0f1d",
   "metadata": {},
   "source": [
    "Define Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8ae0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46dbcf",
   "metadata": {},
   "source": [
    "Build custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f48c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n",
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # Takes whatever arguments needed to build a list of tuples\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "    # Allows the dataset to be indexed, so it can work like a list (dataset[i])\n",
    "    # Either return the corresponding slices of our pre-loaded dataset / tensors or load them on demand\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    # Returns the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "# We dont load our whole training data into GPU tensors, because it takes up space in graphics cards RAM\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a83129",
   "metadata": {},
   "source": [
    "Implement Mini-Batches GD with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970a0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define DataLoader parameters\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85d725b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0977],\n",
       "         [0.1987],\n",
       "         [0.2921],\n",
       "         [0.2809],\n",
       "         [0.2912],\n",
       "         [0.1849],\n",
       "         [0.7290],\n",
       "         [0.7069],\n",
       "         [0.2713],\n",
       "         [0.8084],\n",
       "         [0.3110],\n",
       "         [0.7132],\n",
       "         [0.6075],\n",
       "         [0.5924],\n",
       "         [0.4938],\n",
       "         [0.3117]]),\n",
       " tensor([[1.4417],\n",
       "         [1.2654],\n",
       "         [1.5848],\n",
       "         [1.5846],\n",
       "         [1.4361],\n",
       "         [1.5888],\n",
       "         [2.4927],\n",
       "         [2.4388],\n",
       "         [1.5105],\n",
       "         [2.6141],\n",
       "         [1.5245],\n",
       "         [2.6162],\n",
       "         [2.4037],\n",
       "         [2.1687],\n",
       "         [1.9060],\n",
       "         [1.7637]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a sample mini-batch\n",
    "# List containing two tensors (one for the features, another one for the labels)\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06610002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0229])), ('b', tensor([1.9680]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Inner loop to load each and every mini-batch from our DataLoader\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fa7b1",
   "metadata": {},
   "source": [
    "Implement Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d1f7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8481d3d",
   "metadata": {},
   "source": [
    "Implement Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49be0a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0255])), ('b', tensor([1.9710]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # torch.no_grad() to disable any gradient calculation (gradients belong in training, not in validation steps)\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # eval() sets the model to evaluation mode (just like its train() counterpart did)\n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e89840",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11e79424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n",
      "[1] Training loss: 0.356\t Validation loss: 0.041\n",
      "[2] Training loss: 0.079\t Validation loss: 0.041\n",
      "[3] Training loss: 0.059\t Validation loss: 0.042\n",
      "[4] Training loss: 0.052\t Validation loss: 0.038\n",
      "[5] Training loss: 0.047\t Validation loss: 0.034\n",
      "[6] Training loss: 0.041\t Validation loss: 0.030\n",
      "[7] Training loss: 0.037\t Validation loss: 0.027\n",
      "[8] Training loss: 0.033\t Validation loss: 0.024\n",
      "[9] Training loss: 0.030\t Validation loss: 0.021\n",
      "[10] Training loss: 0.027\t Validation loss: 0.019\n",
      "[11] Training loss: 0.025\t Validation loss: 0.017\n",
      "[12] Training loss: 0.022\t Validation loss: 0.015\n",
      "[13] Training loss: 0.020\t Validation loss: 0.014\n",
      "[14] Training loss: 0.019\t Validation loss: 0.013\n",
      "[15] Training loss: 0.017\t Validation loss: 0.012\n",
      "[16] Training loss: 0.016\t Validation loss: 0.011\n",
      "[17] Training loss: 0.015\t Validation loss: 0.011\n",
      "[18] Training loss: 0.014\t Validation loss: 0.010\n",
      "[19] Training loss: 0.013\t Validation loss: 0.010\n",
      "[20] Training loss: 0.013\t Validation loss: 0.009\n",
      "[21] Training loss: 0.012\t Validation loss: 0.009\n",
      "[22] Training loss: 0.012\t Validation loss: 0.009\n",
      "[23] Training loss: 0.011\t Validation loss: 0.009\n",
      "[24] Training loss: 0.011\t Validation loss: 0.008\n",
      "[25] Training loss: 0.010\t Validation loss: 0.008\n",
      "[26] Training loss: 0.010\t Validation loss: 0.008\n",
      "[27] Training loss: 0.010\t Validation loss: 0.008\n",
      "[28] Training loss: 0.010\t Validation loss: 0.008\n",
      "[29] Training loss: 0.010\t Validation loss: 0.008\n",
      "[30] Training loss: 0.009\t Validation loss: 0.008\n",
      "[31] Training loss: 0.009\t Validation loss: 0.008\n",
      "[32] Training loss: 0.009\t Validation loss: 0.008\n",
      "[33] Training loss: 0.009\t Validation loss: 0.008\n",
      "[34] Training loss: 0.009\t Validation loss: 0.008\n",
      "[35] Training loss: 0.009\t Validation loss: 0.008\n",
      "[36] Training loss: 0.009\t Validation loss: 0.008\n",
      "[37] Training loss: 0.009\t Validation loss: 0.008\n",
      "[38] Training loss: 0.009\t Validation loss: 0.008\n",
      "[39] Training loss: 0.009\t Validation loss: 0.008\n",
      "[40] Training loss: 0.009\t Validation loss: 0.008\n",
      "[41] Training loss: 0.009\t Validation loss: 0.008\n",
      "[42] Training loss: 0.008\t Validation loss: 0.008\n",
      "[43] Training loss: 0.008\t Validation loss: 0.008\n",
      "[44] Training loss: 0.008\t Validation loss: 0.008\n",
      "[45] Training loss: 0.008\t Validation loss: 0.008\n",
      "[46] Training loss: 0.008\t Validation loss: 0.008\n",
      "[47] Training loss: 0.008\t Validation loss: 0.008\n",
      "[48] Training loss: 0.008\t Validation loss: 0.008\n",
      "[49] Training loss: 0.008\t Validation loss: 0.008\n",
      "[50] Training loss: 0.008\t Validation loss: 0.008\n",
      "[51] Training loss: 0.008\t Validation loss: 0.008\n",
      "[52] Training loss: 0.008\t Validation loss: 0.008\n",
      "[53] Training loss: 0.008\t Validation loss: 0.008\n",
      "[54] Training loss: 0.008\t Validation loss: 0.008\n",
      "[55] Training loss: 0.008\t Validation loss: 0.008\n",
      "[56] Training loss: 0.008\t Validation loss: 0.009\n",
      "[57] Training loss: 0.008\t Validation loss: 0.009\n",
      "[58] Training loss: 0.008\t Validation loss: 0.009\n",
      "[59] Training loss: 0.008\t Validation loss: 0.009\n",
      "[60] Training loss: 0.008\t Validation loss: 0.009\n",
      "[61] Training loss: 0.008\t Validation loss: 0.009\n",
      "[62] Training loss: 0.008\t Validation loss: 0.009\n",
      "[63] Training loss: 0.008\t Validation loss: 0.009\n",
      "[64] Training loss: 0.008\t Validation loss: 0.009\n",
      "[65] Training loss: 0.008\t Validation loss: 0.009\n",
      "[66] Training loss: 0.008\t Validation loss: 0.009\n",
      "[67] Training loss: 0.008\t Validation loss: 0.009\n",
      "[68] Training loss: 0.008\t Validation loss: 0.009\n",
      "[69] Training loss: 0.008\t Validation loss: 0.009\n",
      "[70] Training loss: 0.008\t Validation loss: 0.009\n",
      "[71] Training loss: 0.008\t Validation loss: 0.009\n",
      "[72] Training loss: 0.008\t Validation loss: 0.009\n",
      "[73] Training loss: 0.008\t Validation loss: 0.009\n",
      "[74] Training loss: 0.008\t Validation loss: 0.009\n",
      "[75] Training loss: 0.008\t Validation loss: 0.009\n",
      "[76] Training loss: 0.008\t Validation loss: 0.009\n",
      "[77] Training loss: 0.008\t Validation loss: 0.009\n",
      "[78] Training loss: 0.008\t Validation loss: 0.009\n",
      "[79] Training loss: 0.008\t Validation loss: 0.009\n",
      "[80] Training loss: 0.008\t Validation loss: 0.009\n",
      "[81] Training loss: 0.008\t Validation loss: 0.009\n",
      "[82] Training loss: 0.008\t Validation loss: 0.009\n",
      "[83] Training loss: 0.008\t Validation loss: 0.009\n",
      "[84] Training loss: 0.008\t Validation loss: 0.009\n",
      "[85] Training loss: 0.008\t Validation loss: 0.009\n",
      "[86] Training loss: 0.008\t Validation loss: 0.009\n",
      "[87] Training loss: 0.008\t Validation loss: 0.009\n",
      "[88] Training loss: 0.008\t Validation loss: 0.009\n",
      "[89] Training loss: 0.008\t Validation loss: 0.009\n",
      "[90] Training loss: 0.008\t Validation loss: 0.009\n",
      "[91] Training loss: 0.008\t Validation loss: 0.009\n",
      "[92] Training loss: 0.008\t Validation loss: 0.009\n",
      "[93] Training loss: 0.008\t Validation loss: 0.009\n",
      "[94] Training loss: 0.008\t Validation loss: 0.009\n",
      "[95] Training loss: 0.008\t Validation loss: 0.009\n",
      "[96] Training loss: 0.008\t Validation loss: 0.009\n",
      "[97] Training loss: 0.008\t Validation loss: 0.009\n",
      "[98] Training loss: 0.008\t Validation loss: 0.009\n",
      "[99] Training loss: 0.008\t Validation loss: 0.009\n",
      "[100] Training loss: 0.008\t Validation loss: 0.009\n",
      "OrderedDict([('linear.weight', tensor([[1.9750]])), ('linear.bias', tensor([1.0163]))])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "true_a, true_b = 1, 2\n",
    "y = true_a + true_b*x + 0.1*np.random.randn(100, 1)\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor) # dataset = CustomDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(y, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step\n",
    "\n",
    "# Estimate a and b\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = ManualLinearRegression().to(device) # model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 100\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "print(model.state_dict())\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "    training_loss = np.mean(batch_losses)\n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat).item()\n",
    "            val_losses.append(val_loss)\n",
    "        validation_loss = np.mean(val_losses)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
